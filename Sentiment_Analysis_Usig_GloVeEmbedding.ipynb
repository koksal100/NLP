{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwk/VO0Q4Xlj9Tec/lq2S6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koksal100/NLP/blob/main/Sentiment_Analysis_Usig_GloVeEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9IQO-LCCz7gN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import math\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from scipy.sparse import lil_matrix\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus is just a text including a sherlock holmes story\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "    # Küçük harfe çevirme ve boşlukları ayırma\n",
        "    tokens = corpus.lower().split()\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess_corpus(corpus)[:1000]\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfxgnukpz8CB",
        "outputId": "ee1139fd-acdd-443b-d7d0-a5ea8f672035"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cooccurrence_matrix(tokens, window_size=5):\n",
        "    vocab = Counter(tokens)\n",
        "    vocab_size = len(vocab)\n",
        "    index_dict = {word: i for i, word in enumerate(vocab)}\n",
        "    cooccurrence_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float64)\n",
        "\n",
        "    for idx, word in enumerate(tokens):\n",
        "        word_idx = index_dict[word]\n",
        "        start = max(0, idx - window_size)\n",
        "        end = min(len(tokens), idx + window_size + 1)\n",
        "\n",
        "        for i in range(start, end):\n",
        "            if i != idx:\n",
        "                context_word = tokens[i]\n",
        "                context_word_idx = index_dict[context_word]\n",
        "                cooccurrence_matrix[word_idx, context_word_idx] += 1 / abs(i - idx)\n",
        "\n",
        "    return cooccurrence_matrix, index_dict\n",
        "\n",
        "cooccurrence_matrix, index_dict = build_cooccurrence_matrix(tokens)"
      ],
      "metadata": {
        "id": "nl6Mp88Iz9Pr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Explanation\n",
        "\n",
        "\n",
        "Define the Function:\n",
        "Start by defining the build_cooccurrence_matrix function. This function takes two parameters:\n",
        "tokens: A list of words to process.\n",
        "window_size: An integer specifying the context window size around each word (default is 5).\n",
        "\n",
        "\n",
        "Count Word Frequencies:\n",
        "Use the Counter from Python's collections module to count how often each word appears in the tokens list. This helps in building a vocabulary of unique words and their frequencies.\n",
        "\n",
        "\n",
        "Create Word Indices:\n",
        "Create a dictionary (index_dict) that maps each unique word to a numerical index. This index will be used to place words in the co-occurrence matrix.\n",
        "\n",
        "\n",
        "Initialize a Co-occurrence Matrix:\n",
        "Create an empty co-occurrence matrix (cooccurrence_matrix) using lil_matrix from the scipy.sparse module. This matrix will store the co-occurrence counts between each pair of words.\n",
        "\n",
        "\n",
        "\n",
        "Iterate Over Tokens:\n",
        "Loop through the tokens list to process each word (word) and its position index (idx).\n",
        "\n",
        "\n",
        "\n",
        "Determine Context Window:\n",
        "For each word in tokens, determine the range of surrounding words that will form its context. This is defined by start and end indices:\n",
        "start is the maximum of 0 or (idx - window_size), ensuring the window does not go out of bounds.\n",
        "end is the minimum of the length of tokens or (idx + window_size + 1), ensuring the window does not exceed the list length.\n",
        "\n",
        "\n",
        "\n",
        "Iterate Over Context Words:\n",
        "Within the defined context window (start to end), loop through each word (context_word) except the current word (word).\n",
        "\n",
        "\n",
        "\n",
        "Update Co-occurrence Matrix:\n",
        "For each pair of word and context_word, update the corresponding entry in the cooccurrence_matrix. Increase the value by 1 / abs(i - idx), where i is the index of context_word and idx is the index of word. This step captures the strength of association between word and context_word based on their distance within the window.\n",
        "\n",
        "\n",
        "\n",
        "Return Results:After iterating through all words and updating the matrix, return the populated cooccurrence_matrix and the index_dict."
      ],
      "metadata": {
        "id": "W5Y4L5bE9Hkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter(tokens)"
      ],
      "metadata": {
        "id": "Id-qHXaP3YWY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_dict = {word: i for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "ugTJsIpa3kCF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVe:\n",
        "    def __init__(self, vocab_size, embedding_dim=50, x_max=100, alpha=0.75):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.x_max = x_max\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.W = np.random.rand(vocab_size, embedding_dim)\n",
        "        self.W_context = np.random.rand(vocab_size, embedding_dim)\n",
        "        self.b = np.random.rand(vocab_size)\n",
        "        self.b_context = np.random.rand(vocab_size)\n",
        "\n",
        "        self.gradsq_W = np.ones((vocab_size, embedding_dim))\n",
        "        self.gradsq_W_context = np.ones((vocab_size, embedding_dim))\n",
        "        self.gradsq_b = np.ones(vocab_size)\n",
        "        self.gradsq_b_context = np.ones(vocab_size)\n",
        "\n",
        "    def weight_func(self, x):\n",
        "        return (x / self.x_max) ** self.alpha if x < self.x_max else 1\n",
        "\n",
        "    def fit(self, cooccurrence_matrix, index_dict, epochs=100, learning_rate=0.05):\n",
        "        vocab_size = len(index_dict)\n",
        "        for epoch in tqdm.tqdm(range(epochs)):\n",
        "            for i in range(vocab_size):\n",
        "                for j in range(vocab_size):\n",
        "                    if cooccurrence_matrix[i, j] == 0:\n",
        "                        continue\n",
        "                    x_ij = cooccurrence_matrix[i, j]\n",
        "                    weight = self.weight_func(x_ij)\n",
        "                    inner_prod = np.dot(self.W[i], self.W_context[j]) + self.b[i] + self.b_context[j]\n",
        "                    cost = inner_prod - np.log(x_ij)\n",
        "\n",
        "                    self.W[i] -= learning_rate * weight * cost * self.W_context[j] / np.sqrt(self.gradsq_W[i])\n",
        "                    self.W_context[j] -= learning_rate * weight * cost * self.W[i] / np.sqrt(self.gradsq_W_context[j])\n",
        "\n",
        "                    self.b[i] -= learning_rate * weight * cost / np.sqrt(self.gradsq_b[i])\n",
        "                    self.b_context[j] -= learning_rate * weight * cost / np.sqrt(self.gradsq_b_context[j])\n",
        "\n",
        "                    self.gradsq_W[i] += (weight * cost * self.W_context[j]) ** 2\n",
        "                    self.gradsq_W_context[j] += (weight * cost * self.W[i]) ** 2\n",
        "\n",
        "                    self.gradsq_b[i] += (weight * cost) ** 2\n",
        "                    self.gradsq_b_context[j] += (weight * cost) ** 2\n",
        "\n",
        "    def get_embedding(self, word, index_dict):\n",
        "        word_idx = index_dict[word]\n",
        "        return self.W[word_idx] + self.W_context[word_idx]\n"
      ],
      "metadata": {
        "id": "CrPgfX4w3nmY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe modelini başlatma\n",
        "embedding_dim = 50\n",
        "glove_model = GloVe(vocab_size=len(index_dict), embedding_dim=embedding_dim)\n",
        "\n",
        "# Modeli eğitme\n",
        "epochs = 100\n",
        "learning_rate = 0.05\n",
        "glove_model.fit(cooccurrence_matrix, index_dict, epochs=epochs, learning_rate=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1CWkUeh-tAT",
        "outputId": "d18e650f-9a34-43d9-9862-907323220d93"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:43<00:00,  1.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Örnek bir kelimenin embedding vektörünü almak\n",
        "word = 'deduction'\n",
        "embedding_vector = glove_model.get_embedding(word, index_dict)\n",
        "print(f\"Embedding vector for '{word}': {embedding_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CVKmeARBHCh",
        "outputId": "03ce1264-e4a7-47ba-a0e2-d981136a9ced"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding vector for 'deduction': [ 0.58709191  0.23773688 -0.41814824  0.42605251  0.73630965  0.51938128\n",
            "  1.02113996  0.83698164  0.61483124 -0.0607342   1.11388758  0.22387023\n",
            "  1.05113765  0.82584514  0.17191324  0.47326527  0.74657886  0.96291849\n",
            "  0.74271006  1.05681575  0.25061816  0.59595141  0.06347256  0.627788\n",
            "  0.93272076  0.19034006  0.67797183  0.29230095  0.67717746 -0.11653874\n",
            "  0.99198813  1.11758164 -0.29649179  0.30757772 -0.28760836  0.63913467\n",
            "  0.118504    0.50221188  0.55269492  0.58940624  0.8036267  -0.11463196\n",
            "  0.39476471  0.29422352 -0.22400735  0.59716211  0.46941938  0.06097593\n",
            "  1.25990476  0.34266969]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CQL33AkBOJi",
        "outputId": "404dda8f-a24a-452d-aa53-ecc906dec032"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['adventure', 'i.', 'a', 'scandal', 'in', 'bohemia', 'to', 'sherlock', 'holmes', 'she', 'is', 'always', 'the', 'woman.', 'i', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name.', 'his', 'eyes', 'eclipses', 'and', 'predominates', 'whole', 'of', 'sex.', 'it', 'was', 'not', 'that', 'he', 'felt', 'emotion', 'akin', 'love', 'for', 'irene', 'adler.', 'all', 'emotions,', 'one', 'particularly,', 'were', 'abhorrent', 'cold,', 'precise', 'but', 'admirably', 'balanced', 'mind.', 'was,', 'take', 'it,', 'most', 'perfect', 'reasoning', 'observing', 'machine', 'world', 'has', 'seen,', 'as', 'lover', 'would', 'placed', 'himself', 'false', 'position.', 'never', 'spoke', 'softer', 'passions,', 'save', 'with', 'gibe', 'sneer.', 'they', 'admirable', 'things', 'observer--excellent', 'drawing', 'veil', 'from', \"men's\", 'motives', 'actions.', 'trained', 'reasoner', 'admit', 'such', 'intrusions', 'into', 'own', 'delicate', 'finely', 'adjusted', 'temperament', 'introduce', 'distracting', 'factor', 'which', 'might', 'throw', 'doubt', 'upon', 'mental', 'results.', 'grit', 'sensitive', 'instrument,', 'or', 'crack', 'high-power', 'lenses,', 'be', 'more', 'disturbing', 'than', 'strong', 'nature', 'his.', 'yet', 'there', 'woman', 'him,', 'late', 'adler,', 'dubious', 'questionable', 'memory.', 'had', 'seen', 'little', 'lately.', 'my', 'marriage', 'drifted', 'us', 'away', 'each', 'other.', 'complete', 'happiness,', 'home-centred', 'interests', 'rise', 'up', 'around', 'man', 'who', 'first', 'finds', 'master', 'establishment,', 'sufficient', 'absorb', 'attention,', 'while', 'holmes,', 'loathed', 'every', 'form', 'society', 'bohemian', 'soul,', 'remained', 'our', 'lodgings', 'baker', 'street,', 'buried', 'among', 'old', 'books,', 'alternating', 'week', 'between', 'cocaine', 'ambition,', 'drowsiness', 'drug,', 'fierce', 'energy', 'keen', 'nature.', 'still,', 'ever,', 'deeply', 'attracted', 'by', 'study', 'crime,', 'occupied', 'immense', 'faculties', 'extraordinary', 'powers', 'observation', 'following', 'out', 'those', 'clues,', 'clearing', 'mysteries', 'been', 'abandoned', 'hopeless', 'official', 'police.', 'time', 'some', 'vague', 'account', 'doings:', 'summons', 'odessa', 'case', 'trepoff', 'murder,', 'singular', 'tragedy', 'atkinson', 'brothers', 'at', 'trincomalee,', 'finally', 'mission', 'accomplished', 'so', 'delicately', 'successfully', 'reigning', 'family', 'holland.', 'beyond', 'these', 'signs', 'activity,', 'however,', 'merely', 'shared', 'readers', 'daily', 'press,', 'knew', 'former', 'friend', 'companion.', 'night--it', 'on', 'twentieth', 'march,', '1888--i', 'returning', 'journey', 'patient', '(for', 'now', 'returned', 'civil', 'practice),', 'when', 'way', 'led', 'me', 'through', 'street.', 'passed', 'well-remembered', 'door,', 'must', 'associated', 'mind', 'wooing,', 'dark', 'incidents', 'scarlet,', 'seized', 'desire', 'see', 'again,', 'know', 'how', 'employing', 'powers.', 'rooms', 'brilliantly', 'lit,', 'and,', 'even', 'looked', 'up,', 'saw', 'tall,', 'spare', 'figure', 'pass', 'twice', 'silhouette', 'against', 'blind.', 'pacing', 'room', 'swiftly,', 'eagerly,', 'head', 'sunk', 'chest', 'hands', 'clasped', 'behind', 'him.', 'me,', 'mood', 'habit,', 'attitude', 'manner', 'told', 'their', 'story.', 'work', 'again.', 'risen', 'drug-created', 'dreams', 'hot', 'scent', 'new', 'problem.', 'rang', 'bell', 'shown', 'chamber', 'formerly', 'part', 'own.', 'effusive.', 'was;', 'glad,', 'think,', 'me.', 'hardly', 'word', 'spoken,', 'kindly', 'eye,', 'waved', 'an', 'armchair,', 'threw', 'across', 'cigars,', 'indicated', 'spirit', 'gasogene', 'corner.', 'then', 'stood', 'before', 'fire', 'over', 'introspective', 'fashion.', '\"wedlock', 'suits', 'you,\"', 'remarked.', '\"i', 'watson,', 'you', 'put', 'seven', 'half', 'pounds', 'since', 'you.\"', '\"seven!\"', 'answered.', '\"indeed,', 'should', 'thought', 'more.', 'just', 'trifle', 'more,', 'fancy,', 'watson.', 'practice', 'observe.', 'did', 'tell', 'intended', 'go', 'harness.\"', '\"then,', 'do', 'know?\"', 'deduce', 'it.', 'getting', 'yourself', 'very', 'wet', 'lately,', 'clumsy', 'careless', 'servant', 'girl?\"', '\"my', 'dear', 'holmes,\"', 'said', 'i,', '\"this', 'too', 'much.', 'certainly', 'burned,', 'lived', 'few', 'centuries', 'ago.', 'true', 'country', 'walk', 'thursday', 'came', 'home', 'dreadful', 'mess,', 'changed', 'clothes', \"can't\", 'imagine', 'mary', 'jane,', 'incorrigible,', 'wife', 'given', 'notice,', 'there,', 'fail', 'out.\"', 'chuckled', 'rubbed', 'long,', 'nervous', 'together.', '\"it', 'simplicity', 'itself,\"', 'he;', 'inside', 'your', 'left', 'shoe,', 'where', 'firelight', 'strikes', 'leather', 'scored', 'six', 'almost', 'parallel', 'cuts.', 'obviously', 'caused', 'someone', 'carelessly', 'scraped', 'round', 'edges', 'sole', 'order', 'remove', 'crusted', 'mud', 'hence,', 'see,', 'double', 'deduction', 'vile', 'weather,', 'particularly', 'malignant', 'boot-slitting', 'specimen', 'london', 'slavey.', 'practice,', 'if', 'gentleman', 'walks', 'smelling', 'iodoform,', 'black', 'mark', 'nitrate', 'silver', 'right', 'forefinger,', 'bulge', 'side', 'top-hat', 'show', 'secreted', 'stethoscope,'])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sentiment Analysis Modeli\n",
        "class SentimentAnalysisModel:\n",
        "    def __init__(self, glove_model, index_dict):\n",
        "        self.glove_model = glove_model\n",
        "        self.index_dict = index_dict\n",
        "        self.logreg = LogisticRegression()\n",
        "\n",
        "    def get_features(self, sentence):\n",
        "        tokens = sentence.lower().split()\n",
        "        sentence_embedding = np.zeros(self.glove_model.embedding_dim)\n",
        "        for token in tokens:\n",
        "            if token in self.index_dict:\n",
        "                word_idx = self.index_dict[token]\n",
        "                sentence_embedding += self.glove_model.get_embedding(token, self.index_dict)\n",
        "        return sentence_embedding.reshape(1, -1)\n",
        "\n",
        "    def train(self, labeled_sentences):\n",
        "        X_train = []\n",
        "        y_train = []\n",
        "\n",
        "        for sentence, label in labeled_sentences:\n",
        "            tokens = sentence.lower().split()\n",
        "            sentence_embedding = self.get_features(sentence)\n",
        "            X_train.append(sentence_embedding)\n",
        "            y_train.append(label)\n",
        "\n",
        "        X_train = np.vstack(X_train)\n",
        "        self.logreg.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        X_pred = []\n",
        "        for sentence in sentences:\n",
        "            tokens = sentence.lower().split()\n",
        "            sentence_embedding = self.get_features(sentence)\n",
        "            X_pred.append(sentence_embedding)\n",
        "\n",
        "        X_pred = np.vstack(X_pred)\n",
        "        predictions = self.logreg.predict(X_pred)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "Bh0t0n3OGomo"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_sentences = [\n",
        "    (\"Holmes' methods draw a veil from men's motives.\", 0),\n",
        "    (\"His finely adjusted temperament is disturbed by emotional intrusions.\", 1),\n",
        "    (\"He loathed every form of society.\", 1),\n",
        "    (\"Holmes' abhorrence of emotions was evident in his sneer.\", 0),\n",
        "    (\"He saw the signs of activity in the reign of Holland.\", 1),\n",
        "    (\"Holmes' observations were particularly acute.\", 1),\n",
        "    (\"The mysterious case of Trepoff's murder baffled official police.\", 1),\n",
        "    (\"Holmes' balanced mind admired Irene Adler's courage.\", 1),\n",
        "    (\"His deep emotions were abhorrent to his cold, precise mind.\", 0),\n",
        "    (\"Sherlock Holmes is deeply absorbed in the study of crime.\", 1),\n",
        "    (\"He has a deep aversion to softer passions.\", 0),\n",
        "    (\"His complete happiness was home-centered.\", 1),\n",
        "    (\"The crack in the lens could throw doubt on his mental results.\", 0),\n",
        "    (\"Holmes remained immersed in his work.\", 1),\n",
        "    (\"The woman, Irene Adler, was a singular and tragic figure.\", 0),\n",
        "    (\"He remained buried among old books in his lodgings.\", 1),\n",
        "    (\"Holmes' loathing of society contrasted sharply with Watson's habits.\", 1),\n",
        "    (\"His balanced mind admired Irene Adler's courage.\", 1),\n",
        "    (\"Holmes' abhorrence of emotions was evident in his sneer.\", 0),\n",
        "    (\"He has a deep aversion to softer passions.\", 0),\n",
        "    (\"His complete happiness was home-centered.\", 1),\n",
        "    (\"The crack in the lens could throw doubt on his mental results.\", 0),\n",
        "    (\"Holmes remained immersed in his work.\", 1),\n",
        "    (\"The woman, Irene Adler, was a singular and tragic figure.\", 0),\n",
        "    (\"He remained buried among old books in his lodgings.\", 1),\n",
        "    (\"Holmes' loathing of society contrasted sharply with Watson's habits.\", 1),\n",
        "    (\"His balanced mind admired Irene Adler's courage.\", 1),\n",
        "    (\"Holmes' abhorrence of emotions was evident in his sneer.\", 0),\n",
        "    (\"He has a deep aversion to softer passions.\", 0),\n",
        "    (\"His complete happiness was home-centered.\", 1)\n",
        "]"
      ],
      "metadata": {
        "id": "NzxaErmPKCAo"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis modelini oluşturma ve eğitim yapma\n",
        "sentiment_model = SentimentAnalysisModel(glove_model, index_dict)\n",
        "sentiment_model.train(labeled_sentences[:20])\n",
        "\n"
      ],
      "metadata": {
        "id": "1h9oL08MIPu5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=[label for sentence,label in labeled_sentences[20:]]\n",
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3qMhPd3LYoZ",
        "outputId": "c2996e0b-12a9-49f4-8a0f-4aba02bea394"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1, 0, 1, 1, 1, 0, 0, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test =[sentence for sentence,label in labeled_sentences[20:]]\n",
        "x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFr7QuqtIPRe",
        "outputId": "c9ce80c4-cb2b-4afe-8f09-be759723dc12"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['His complete happiness was home-centered.',\n",
              " 'The crack in the lens could throw doubt on his mental results.',\n",
              " 'Holmes remained immersed in his work.',\n",
              " 'The woman, Irene Adler, was a singular and tragic figure.',\n",
              " 'He remained buried among old books in his lodgings.',\n",
              " \"Holmes' loathing of society contrasted sharply with Watson's habits.\",\n",
              " \"His balanced mind admired Irene Adler's courage.\",\n",
              " \"Holmes' abhorrence of emotions was evident in his sneer.\",\n",
              " 'He has a deep aversion to softer passions.',\n",
              " 'His complete happiness was home-centered.']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=sentiment_model.predict(x_test)"
      ],
      "metadata": {
        "id": "WxQ1R-oeLX3G"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcDtFkRCL0Ek",
        "outputId": "6db6f215-f4f9-4183-e4a7-61431bba485a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}