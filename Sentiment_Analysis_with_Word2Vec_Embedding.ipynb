{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt6GI8VVFCaBL50f9nIuaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koksal100/NLP/blob/main/Sentiment_Analysis_with_Word2Vec_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WAYS FOR CREATING EMBEDDING**\n",
        "1. Word2Vec\n",
        "Word2Vec is a widely used method for generating word embeddings. It includes two models: Skip-gram and Continuous Bag of Words (CBOW). Skip-gram predicts the context around a word to learn its embedding vector, while CBOW predicts the word itself given its context. Word2Vec models are typically trained on large text corpora and the resulting word embeddings are used in natural language processing tasks.\n",
        "\n",
        "2. GloVe (Global Vectors for Word Representation)\n",
        "GloVe is another popular method for word embeddings developed by Stanford University. GloVe calculates word embeddings based on the co-occurrence statistics of words. It uses statistics from large text datasets to learn word vectors.\n",
        "\n",
        "3. FastText\n",
        "FastText is a word embedding model developed by Facebook. It breaks down each word into character n-grams and averages the vectors of these n-grams to create the word embedding vector. This method is effective in capturing morphological features and semantic meanings of subwords.\n",
        "\n",
        "4. ELMo (Embeddings from Language Models)\n",
        "ELMo generates word embeddings using deep learning models. It combines outputs from language models and weights them to create word embeddings. ELMo considers both previous and subsequent words to better understand the context of a word.\n",
        "\n",
        "5. BERT (Bidirectional Encoder Representations from Transformers)\n",
        "BERT is a language model developed by Google that not only creates word embeddings but also excels in various natural language processing tasks. It is pretrained on large text corpora and uses bidirectional transformers to capture context and semantics effectively.\n",
        "\n",
        "6. Transformer Encoder\n",
        "Transformer Encoder is an architecture designed for natural language processing tasks, including creating word embeddings. It utilizes the attention mechanism to generate word embeddings and is particularly effective for handling long-range dependencies and parallel processing."
      ],
      "metadata": {
        "id": "yA0xPskQhGj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "slIzO5BpOaee"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"The quick brown fox jumped over the lazy dog, surprising everyone in the park.\",\n",
        "    \"I learned Python programming in university, which opened up a world of possibilities.\",\n",
        "    \"Artificial intelligence continues to revolutionize industries around the globe.\",\n",
        "    \"The majestic mountains were covered in a blanket of fresh, powdery snow.\",\n",
        "    \"She meticulously planned every detail of her upcoming wedding.\",\n",
        "    \"In the bustling city, the sound of honking horns filled the air.\",\n",
        "    \"He was a brilliant scientist who dedicated his life to researching rare diseases.\",\n",
        "    \"The ancient ruins stood silently, telling tales of civilizations long gone.\",\n",
        "    \"They embarked on a thrilling adventure across the rugged terrain.\",\n",
        "    \"The sun set behind the horizon, painting the sky in hues of orange and pink.\",\n",
        "    \"Her contagious laughter filled the room, brightening everyone's mood.\",\n",
        "    \"We gathered around the campfire, sharing stories under the starry night sky.\",\n",
        "    \"The intricate design of the cathedral left visitors in awe of its beauty.\",\n",
        "    \"After years of hard work, she finally achieved her dream of becoming a published author.\",\n",
        "    \"The stormy seas tossed the ship to and fro, testing the sailors' resolve.\",\n",
        "    \"He found solace in the pages of his favorite book, escaping into fantastical worlds.\",\n",
        "    \"The bustling market was a feast for the senses, with vibrant colors and enticing aromas.\",\n",
        "    \"They trekked through dense jungles, discovering hidden treasures along the way.\",\n",
        "    \"The melody of the piano echoed through the concert hall, captivating the audience.\",\n",
        "    \"She embarked on a culinary journey, mastering the art of French cuisine.\"\n",
        "]"
      ],
      "metadata": {
        "id": "wAKgZuGEOe9H"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word2index = tokenizer.word_index\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "vocab_size = len(word2index) + 1\n"
      ],
      "metadata": {
        "id": "nYXQ54jzOgzr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5QJ8zZnOjIo",
        "outputId": "b9d5e09f-0c69-4564-ff48-3f638e65f2c1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'of': 2,\n",
              " 'in': 3,\n",
              " 'a': 4,\n",
              " 'to': 5,\n",
              " 'she': 6,\n",
              " 'her': 7,\n",
              " 'and': 8,\n",
              " 'around': 9,\n",
              " 'bustling': 10,\n",
              " 'filled': 11,\n",
              " 'he': 12,\n",
              " 'was': 13,\n",
              " 'his': 14,\n",
              " 'they': 15,\n",
              " 'embarked': 16,\n",
              " 'on': 17,\n",
              " 'sky': 18,\n",
              " 'through': 19,\n",
              " 'quick': 20,\n",
              " 'brown': 21,\n",
              " 'fox': 22,\n",
              " 'jumped': 23,\n",
              " 'over': 24,\n",
              " 'lazy': 25,\n",
              " 'dog': 26,\n",
              " 'surprising': 27,\n",
              " 'everyone': 28,\n",
              " 'park': 29,\n",
              " 'i': 30,\n",
              " 'learned': 31,\n",
              " 'python': 32,\n",
              " 'programming': 33,\n",
              " 'university': 34,\n",
              " 'which': 35,\n",
              " 'opened': 36,\n",
              " 'up': 37,\n",
              " 'world': 38,\n",
              " 'possibilities': 39,\n",
              " 'artificial': 40,\n",
              " 'intelligence': 41,\n",
              " 'continues': 42,\n",
              " 'revolutionize': 43,\n",
              " 'industries': 44,\n",
              " 'globe': 45,\n",
              " 'majestic': 46,\n",
              " 'mountains': 47,\n",
              " 'were': 48,\n",
              " 'covered': 49,\n",
              " 'blanket': 50,\n",
              " 'fresh': 51,\n",
              " 'powdery': 52,\n",
              " 'snow': 53,\n",
              " 'meticulously': 54,\n",
              " 'planned': 55,\n",
              " 'every': 56,\n",
              " 'detail': 57,\n",
              " 'upcoming': 58,\n",
              " 'wedding': 59,\n",
              " 'city': 60,\n",
              " 'sound': 61,\n",
              " 'honking': 62,\n",
              " 'horns': 63,\n",
              " 'air': 64,\n",
              " 'brilliant': 65,\n",
              " 'scientist': 66,\n",
              " 'who': 67,\n",
              " 'dedicated': 68,\n",
              " 'life': 69,\n",
              " 'researching': 70,\n",
              " 'rare': 71,\n",
              " 'diseases': 72,\n",
              " 'ancient': 73,\n",
              " 'ruins': 74,\n",
              " 'stood': 75,\n",
              " 'silently': 76,\n",
              " 'telling': 77,\n",
              " 'tales': 78,\n",
              " 'civilizations': 79,\n",
              " 'long': 80,\n",
              " 'gone': 81,\n",
              " 'thrilling': 82,\n",
              " 'adventure': 83,\n",
              " 'across': 84,\n",
              " 'rugged': 85,\n",
              " 'terrain': 86,\n",
              " 'sun': 87,\n",
              " 'set': 88,\n",
              " 'behind': 89,\n",
              " 'horizon': 90,\n",
              " 'painting': 91,\n",
              " 'hues': 92,\n",
              " 'orange': 93,\n",
              " 'pink': 94,\n",
              " 'contagious': 95,\n",
              " 'laughter': 96,\n",
              " 'room': 97,\n",
              " 'brightening': 98,\n",
              " \"everyone's\": 99,\n",
              " 'mood': 100,\n",
              " 'we': 101,\n",
              " 'gathered': 102,\n",
              " 'campfire': 103,\n",
              " 'sharing': 104,\n",
              " 'stories': 105,\n",
              " 'under': 106,\n",
              " 'starry': 107,\n",
              " 'night': 108,\n",
              " 'intricate': 109,\n",
              " 'design': 110,\n",
              " 'cathedral': 111,\n",
              " 'left': 112,\n",
              " 'visitors': 113,\n",
              " 'awe': 114,\n",
              " 'its': 115,\n",
              " 'beauty': 116,\n",
              " 'after': 117,\n",
              " 'years': 118,\n",
              " 'hard': 119,\n",
              " 'work': 120,\n",
              " 'finally': 121,\n",
              " 'achieved': 122,\n",
              " 'dream': 123,\n",
              " 'becoming': 124,\n",
              " 'published': 125,\n",
              " 'author': 126,\n",
              " 'stormy': 127,\n",
              " 'seas': 128,\n",
              " 'tossed': 129,\n",
              " 'ship': 130,\n",
              " 'fro': 131,\n",
              " 'testing': 132,\n",
              " \"sailors'\": 133,\n",
              " 'resolve': 134,\n",
              " 'found': 135,\n",
              " 'solace': 136,\n",
              " 'pages': 137,\n",
              " 'favorite': 138,\n",
              " 'book': 139,\n",
              " 'escaping': 140,\n",
              " 'into': 141,\n",
              " 'fantastical': 142,\n",
              " 'worlds': 143,\n",
              " 'market': 144,\n",
              " 'feast': 145,\n",
              " 'for': 146,\n",
              " 'senses': 147,\n",
              " 'with': 148,\n",
              " 'vibrant': 149,\n",
              " 'colors': 150,\n",
              " 'enticing': 151,\n",
              " 'aromas': 152,\n",
              " 'trekked': 153,\n",
              " 'dense': 154,\n",
              " 'jungles': 155,\n",
              " 'discovering': 156,\n",
              " 'hidden': 157,\n",
              " 'treasures': 158,\n",
              " 'along': 159,\n",
              " 'way': 160,\n",
              " 'melody': 161,\n",
              " 'piano': 162,\n",
              " 'echoed': 163,\n",
              " 'concert': 164,\n",
              " 'hall': 165,\n",
              " 'captivating': 166,\n",
              " 'audience': 167,\n",
              " 'culinary': 168,\n",
              " 'journey': 169,\n",
              " 'mastering': 170,\n",
              " 'art': 171,\n",
              " 'french': 172,\n",
              " 'cuisine': 173}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW6x2-YsOl1a",
        "outputId": "f6be5714-f3b8-436e-e380-41c37d1aa072"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsxSkQCJOtLo",
        "outputId": "8a6bbf39-6fa7-4f6a-8804-fd2428e9bb36"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 20, 21, 22, 23, 24, 1, 25, 26, 27, 28, 3, 1, 29],\n",
              " [30, 31, 32, 33, 3, 34, 35, 36, 37, 4, 38, 2, 39],\n",
              " [40, 41, 42, 5, 43, 44, 9, 1, 45],\n",
              " [1, 46, 47, 48, 49, 3, 4, 50, 2, 51, 52, 53],\n",
              " [6, 54, 55, 56, 57, 2, 7, 58, 59],\n",
              " [3, 1, 10, 60, 1, 61, 2, 62, 63, 11, 1, 64],\n",
              " [12, 13, 4, 65, 66, 67, 68, 14, 69, 5, 70, 71, 72],\n",
              " [1, 73, 74, 75, 76, 77, 78, 2, 79, 80, 81],\n",
              " [15, 16, 17, 4, 82, 83, 84, 1, 85, 86],\n",
              " [1, 87, 88, 89, 1, 90, 91, 1, 18, 3, 92, 2, 93, 8, 94],\n",
              " [7, 95, 96, 11, 1, 97, 98, 99, 100],\n",
              " [101, 102, 9, 1, 103, 104, 105, 106, 1, 107, 108, 18],\n",
              " [1, 109, 110, 2, 1, 111, 112, 113, 3, 114, 2, 115, 116],\n",
              " [117, 118, 2, 119, 120, 6, 121, 122, 7, 123, 2, 124, 4, 125, 126],\n",
              " [1, 127, 128, 129, 1, 130, 5, 8, 131, 132, 1, 133, 134],\n",
              " [12, 135, 136, 3, 1, 137, 2, 14, 138, 139, 140, 141, 142, 143],\n",
              " [1, 10, 144, 13, 4, 145, 146, 1, 147, 148, 149, 150, 8, 151, 152],\n",
              " [15, 153, 19, 154, 155, 156, 157, 158, 159, 1, 160],\n",
              " [1, 161, 2, 1, 162, 163, 19, 1, 164, 165, 166, 1, 167],\n",
              " [6, 16, 17, 4, 168, 169, 170, 1, 171, 2, 172, 173]]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "skip_grams = [skipgrams(sequence, vocabulary_size=vocab_size, window_size=window_size) for sequence in sequences]"
      ],
      "metadata": {
        "id": "HGHj7m6VQ6TJ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#return word tuples and 0/1 regarding these tuple have a positive relationship or not."
      ],
      "metadata": {
        "id": "teJnsQFUWvHB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 100\n",
        "\n",
        "# Modelin giriş katmanları\n",
        "input_target = tf.keras.layers.Input((1,))\n",
        "input_context = tf.keras.layers.Input((1,))\n",
        "\n",
        "embedding = tf.keras.layers.Embedding(vocab_size, embed_size, input_length=1, name='embedding')\n",
        "\n",
        "target = embedding(input_target)\n",
        "target = tf.keras.layers.Reshape((embed_size,))(target)\n",
        "context = embedding(input_context)\n",
        "context = tf.keras.layers.Reshape((embed_size,))(context)\n",
        "\n",
        "dot_product = tf.keras.layers.Dot(axes=1)([target, context])\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[input_target, input_context], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Eğitim verilerini hazırlama ve eğitme\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for target_context, label in skip_grams:\n",
        "        if len(target_context) == 0:\n",
        "            continue\n",
        "        target_array = np.array([pair[0] for pair in target_context], dtype='int32')\n",
        "        context_array = np.array([pair[1] for pair in target_context], dtype='int32')\n",
        "        labels = np.array(label, dtype='int32')\n",
        "        X = [target_array, context_array]\n",
        "        Y = labels\n",
        "        loss += model.train_on_batch(X, Y)\n",
        "    print(f'Epoch: {epoch+1}, Loss: {loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnlC-sqOWwVB",
        "outputId": "f1fc2b43-dc32-47d1-8cbe-b238b9b4280a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 13.866081357002258\n",
            "Epoch: 2, Loss: 13.795098960399628\n",
            "Epoch: 3, Loss: 13.721691250801086\n",
            "Epoch: 4, Loss: 13.623933136463165\n",
            "Epoch: 5, Loss: 13.489246666431427\n",
            "Epoch: 6, Loss: 13.305572271347046\n",
            "Epoch: 7, Loss: 13.06225198507309\n",
            "Epoch: 8, Loss: 12.75157880783081\n",
            "Epoch: 9, Loss: 12.370360493659973\n",
            "Epoch: 10, Loss: 11.920932829380035\n",
            "Epoch: 11, Loss: 11.411242246627808\n",
            "Epoch: 12, Loss: 10.85390168428421\n",
            "Epoch: 13, Loss: 10.264495521783829\n",
            "Epoch: 14, Loss: 9.659625113010406\n",
            "Epoch: 15, Loss: 9.055167555809021\n",
            "Epoch: 16, Loss: 8.464980781078339\n",
            "Epoch: 17, Loss: 7.900113254785538\n",
            "Epoch: 18, Loss: 7.368484348058701\n",
            "Epoch: 19, Loss: 6.87497091293335\n",
            "Epoch: 20, Loss: 6.421793594956398\n",
            "Epoch: 21, Loss: 6.009070664644241\n",
            "Epoch: 22, Loss: 5.635419055819511\n",
            "Epoch: 23, Loss: 5.298501417040825\n",
            "Epoch: 24, Loss: 4.9954639077186584\n",
            "Epoch: 25, Loss: 4.723259173333645\n",
            "Epoch: 26, Loss: 4.478859268128872\n",
            "Epoch: 27, Loss: 4.2593841180205345\n",
            "Epoch: 28, Loss: 4.062168516218662\n",
            "Epoch: 29, Loss: 3.8847927153110504\n",
            "Epoch: 30, Loss: 3.7250813469290733\n",
            "Epoch: 31, Loss: 3.5810935720801353\n",
            "Epoch: 32, Loss: 3.451104648411274\n",
            "Epoch: 33, Loss: 3.333585198968649\n",
            "Epoch: 34, Loss: 3.227180738002062\n",
            "Epoch: 35, Loss: 3.1306899897754192\n",
            "Epoch: 36, Loss: 3.0430486239492893\n",
            "Epoch: 37, Loss: 2.963313102722168\n",
            "Epoch: 38, Loss: 2.890645783394575\n",
            "Epoch: 39, Loss: 2.824303213506937\n",
            "Epoch: 40, Loss: 2.763625767081976\n",
            "Epoch: 41, Loss: 2.708027206361294\n",
            "Epoch: 42, Loss: 2.6569864824414253\n",
            "Epoch: 43, Loss: 2.610040817409754\n",
            "Epoch: 44, Loss: 2.56677895411849\n",
            "Epoch: 45, Loss: 2.5268353540450335\n",
            "Epoch: 46, Loss: 2.4898845590651035\n",
            "Epoch: 47, Loss: 2.455636901780963\n",
            "Epoch: 48, Loss: 2.4238338470458984\n",
            "Epoch: 49, Loss: 2.3942461842671037\n",
            "Epoch: 50, Loss: 2.3666682820767164\n",
            "Epoch: 51, Loss: 2.340916907414794\n",
            "Epoch: 52, Loss: 2.3168285880237818\n",
            "Epoch: 53, Loss: 2.29425626154989\n",
            "Epoch: 54, Loss: 2.2730683339759707\n",
            "Epoch: 55, Loss: 2.2531466661021113\n",
            "Epoch: 56, Loss: 2.234385432675481\n",
            "Epoch: 57, Loss: 2.2166891265660524\n",
            "Epoch: 58, Loss: 2.199971734546125\n",
            "Epoch: 59, Loss: 2.1841558143496513\n",
            "Epoch: 60, Loss: 2.169171311892569\n",
            "Epoch: 61, Loss: 2.1549545023590326\n",
            "Epoch: 62, Loss: 2.141447974834591\n",
            "Epoch: 63, Loss: 2.1285999822430313\n",
            "Epoch: 64, Loss: 2.116362669505179\n",
            "Epoch: 65, Loss: 2.104692821390927\n",
            "Epoch: 66, Loss: 2.093551252502948\n",
            "Epoch: 67, Loss: 2.0829019700177014\n",
            "Epoch: 68, Loss: 2.072712139226496\n",
            "Epoch: 69, Loss: 2.0629519051872194\n",
            "Epoch: 70, Loss: 2.053593291901052\n",
            "Epoch: 71, Loss: 2.044611136894673\n",
            "Epoch: 72, Loss: 2.0359822935424745\n",
            "Epoch: 73, Loss: 2.0276849935762584\n",
            "Epoch: 74, Loss: 2.0196998296305537\n",
            "Epoch: 75, Loss: 2.012008403427899\n",
            "Epoch: 76, Loss: 2.004593959543854\n",
            "Epoch: 77, Loss: 1.9974408699199557\n",
            "Epoch: 78, Loss: 1.990534690208733\n",
            "Epoch: 79, Loss: 1.983861951623112\n",
            "Epoch: 80, Loss: 1.977410351159051\n",
            "Epoch: 81, Loss: 1.9711683578789234\n",
            "Epoch: 82, Loss: 1.9651248261798173\n",
            "Epoch: 83, Loss: 1.9592699082568288\n",
            "Epoch: 84, Loss: 1.9535944655071944\n",
            "Epoch: 85, Loss: 1.9480895611923188\n",
            "Epoch: 86, Loss: 1.9427469589281827\n",
            "Epoch: 87, Loss: 1.9375590917188674\n",
            "Epoch: 88, Loss: 1.9325186354108155\n",
            "Epoch: 89, Loss: 1.9276191061362624\n",
            "Epoch: 90, Loss: 1.922854102216661\n",
            "Epoch: 91, Loss: 1.9182177486363798\n",
            "Epoch: 92, Loss: 1.9137041517533362\n",
            "Epoch: 93, Loss: 1.9093083683401346\n",
            "Epoch: 94, Loss: 1.905025368789211\n",
            "Epoch: 95, Loss: 1.9008504929952323\n",
            "Epoch: 96, Loss: 1.8967793048359454\n",
            "Epoch: 97, Loss: 1.8928076797164977\n",
            "Epoch: 98, Loss: 1.888931632740423\n",
            "Epoch: 99, Loss: 1.885147530818358\n",
            "Epoch: 100, Loss: 1.8814517084974796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skip_grams[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNJGwFoRaCig",
        "outputId": "c264f923-f70a-41c9-f4e6-e0e5c7986364"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[1, 19],\n",
              "  [26, 28],\n",
              "  [21, 107],\n",
              "  [24, 155],\n",
              "  [3, 102],\n",
              "  [27, 26],\n",
              "  [1, 1],\n",
              "  [1, 9],\n",
              "  [28, 31],\n",
              "  [22, 111],\n",
              "  [1, 21],\n",
              "  [28, 1],\n",
              "  [27, 127],\n",
              "  [3, 71],\n",
              "  [1, 30],\n",
              "  [20, 22],\n",
              "  [29, 164],\n",
              "  [22, 20],\n",
              "  [27, 28],\n",
              "  [1, 25],\n",
              "  [1, 20],\n",
              "  [29, 1],\n",
              "  [28, 33],\n",
              "  [24, 25],\n",
              "  [25, 3],\n",
              "  [26, 154],\n",
              "  [26, 1],\n",
              "  [28, 146],\n",
              "  [26, 25],\n",
              "  [28, 155],\n",
              "  [21, 20],\n",
              "  [23, 21],\n",
              "  [20, 128],\n",
              "  [3, 27],\n",
              "  [27, 25],\n",
              "  [27, 122],\n",
              "  [1, 23],\n",
              "  [1, 51],\n",
              "  [22, 155],\n",
              "  [20, 47],\n",
              "  [25, 72],\n",
              "  [1, 33],\n",
              "  [25, 26],\n",
              "  [25, 135],\n",
              "  [23, 151],\n",
              "  [1, 29],\n",
              "  [26, 19],\n",
              "  [1, 28],\n",
              "  [27, 3],\n",
              "  [28, 26],\n",
              "  [1, 3],\n",
              "  [24, 22],\n",
              "  [20, 1],\n",
              "  [1, 42],\n",
              "  [23, 77],\n",
              "  [20, 21],\n",
              "  [24, 17],\n",
              "  [21, 22],\n",
              "  [23, 148],\n",
              "  [1, 26],\n",
              "  [24, 23],\n",
              "  [25, 1],\n",
              "  [28, 27],\n",
              "  [21, 21],\n",
              "  [21, 1],\n",
              "  [23, 101],\n",
              "  [3, 42],\n",
              "  [21, 25],\n",
              "  [24, 160],\n",
              "  [3, 28],\n",
              "  [24, 1],\n",
              "  [29, 142],\n",
              "  [22, 24],\n",
              "  [26, 47],\n",
              "  [3, 1],\n",
              "  [26, 69],\n",
              "  [25, 71],\n",
              "  [21, 23],\n",
              "  [1, 24],\n",
              "  [24, 104],\n",
              "  [1, 34],\n",
              "  [21, 14],\n",
              "  [3, 31],\n",
              "  [26, 27],\n",
              "  [25, 24],\n",
              "  [28, 3],\n",
              "  [27, 68],\n",
              "  [1, 171],\n",
              "  [22, 44],\n",
              "  [22, 21],\n",
              "  [23, 24],\n",
              "  [20, 133],\n",
              "  [22, 23],\n",
              "  [25, 27],\n",
              "  [22, 45],\n",
              "  [23, 22],\n",
              "  [29, 3],\n",
              "  [3, 29],\n",
              "  [23, 1],\n",
              "  [27, 37]],\n",
              " [0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model.get_layer('embedding').get_weights()[0]"
      ],
      "metadata": {
        "id": "ScGX2wYIbuge"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPRYPSvNiMoz",
        "outputId": "4b09e10e-b2a0-4a1d-c197-9219b053d112"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(174, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THERE IS A 100 NUMERS LENGTH VECTOR FOR EACH WORD IN THE WORD_VECTORS EMBEDDINGS**"
      ],
      "metadata": {
        "id": "tcQvQ1NOidGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cümleler listesi\n",
        "sentences = [\n",
        "    \"The brilliant scientist dedicated his life to researching rare diseases.\",\n",
        "    \"Stormy seas tossed the ship, testing the sailors' resolve.\",\n",
        "    \"Majestic mountains were covered in a blanket of fresh powdery snow.\",\n",
        "    \"The bustling city, with its constant sound of honking horns, was overwhelming.\",\n",
        "    \"The intricate design of the cathedral left visitors in awe of its beauty.\",\n",
        "    \"The market was a feast for the senses but left me feeling dizzy from the vibrant colors and enticing aromas.\",\n",
        "    \"After years of hard work, she finally achieved her dream of becoming a published author.\",\n",
        "    \"She meticulously planned every detail of the upcoming wedding, but things still seemed to go wrong.\",\n",
        "    \"The thrilling adventure across rugged terrain was an unforgettable experience.\",\n",
        "    \"The ancient ruins stood silently, telling tales of civilizations long gone, which was rather melancholic.\",\n",
        "    \"The sun set behind the horizon, painting the sky in hues of orange and pink.\",\n",
        "    \"The sky was overcast and gray, casting a gloomy shadow over the city.\",\n",
        "    \"Contagious laughter filled the room, brightening everyone's mood.\",\n",
        "    \"Despite his efforts, he couldn't escape the feeling of failure after the project fell through.\",\n",
        "    \"We gathered around the campfire, sharing stories under the starry night.\",\n",
        "    \"The dense jungles were trekked through, but they seemed to hold more dangers than anticipated.\",\n",
        "    \"The melody of the piano echoed through the concert hall, captivating the audience.\",\n",
        "    \"The room fell silent after hearing the news of the accident, leaving everyone in a state of shock.\",\n",
        "    \"The culinary journey included mastering the art of French cuisine.\",\n",
        "    \"The shipwreck was a tragic reminder of the dangers lurking in the stormy seas.\"\n",
        "]\n",
        "\n",
        "# Etiketler listesi (1: pozitif, 0: negatif)\n",
        "labels = [\n",
        "    1,  # \"The brilliant scientist dedicated his life to researching rare diseases.\"\n",
        "    0,  # \"Stormy seas tossed the ship, testing the sailors' resolve.\"\n",
        "    1,  # \"Majestic mountains were covered in a blanket of fresh powdery snow.\"\n",
        "    0,  # \"The bustling city, with its constant sound of honking horns, was overwhelming.\"\n",
        "    1,  # \"The intricate design of the cathedral left visitors in awe of its beauty.\"\n",
        "    0,  # \"The market was a feast for the senses but left me feeling dizzy from the vibrant colors and enticing aromas.\"\n",
        "    1,  # \"After years of hard work, she finally achieved her dream of becoming a published author.\"\n",
        "    0,  # \"She meticulously planned every detail of the upcoming wedding, but things still seemed to go wrong.\"\n",
        "    1,  # \"The thrilling adventure across rugged terrain was an unforgettable experience.\"\n",
        "    0,  # \"The ancient ruins stood silently, telling tales of civilizations long gone, which was rather melancholic.\"\n",
        "    1,  # \"The sun set behind the horizon, painting the sky in hues of orange and pink.\"\n",
        "    0,  # \"The sky was overcast and gray, casting a gloomy shadow over the city.\"\n",
        "    1,  # \"Contagious laughter filled the room, brightening everyone's mood.\"\n",
        "    0,  # \"Despite his efforts, he couldn't escape the feeling of failure after the project fell through.\"\n",
        "    1,  # \"We gathered around the campfire, sharing stories under the starry night.\"\n",
        "    0,  # \"The dense jungles were trekked through, but they seemed to hold more dangers than anticipated.\"\n",
        "    1,  # \"The melody of the piano echoed through the concert hall, captivating the audience.\"\n",
        "    0,  # \"The room fell silent after hearing the news of the accident, leaving everyone in a state of shock.\"\n",
        "    1,  # \"The culinary journey included mastering the art of French cuisine.\"\n",
        "    0   # \"The shipwreck was a tragic reminder of the dangers lurking in the stormy seas.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "hf3t3QLofjL8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9yFb4Wwfl6t",
        "outputId": "ac25037e-ea13-47c1-fdc8-b6ea0089cda5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 65, 66, 68, 14, 69, 5, 70, 71, 72],\n",
              " [127, 128, 129, 1, 130, 132, 1, 133, 134],\n",
              " [46, 47, 48, 49, 3, 4, 50, 2, 51, 52, 53],\n",
              " [1, 10, 60, 148, 115, 61, 2, 62, 63, 13],\n",
              " [1, 109, 110, 2, 1, 111, 112, 113, 3, 114, 2, 115, 116],\n",
              " [1, 144, 13, 4, 145, 146, 1, 147, 112, 1, 149, 150, 8, 151, 152],\n",
              " [117, 118, 2, 119, 120, 6, 121, 122, 7, 123, 2, 124, 4, 125, 126],\n",
              " [6, 54, 55, 56, 57, 2, 1, 58, 59, 5],\n",
              " [1, 82, 83, 84, 85, 86, 13],\n",
              " [1, 73, 74, 75, 76, 77, 78, 2, 79, 80, 81, 35, 13],\n",
              " [1, 87, 88, 89, 1, 90, 91, 1, 18, 3, 92, 2, 93, 8, 94],\n",
              " [1, 18, 13, 8, 4, 24, 1, 60],\n",
              " [95, 96, 11, 1, 97, 98, 99, 100],\n",
              " [14, 12, 1, 2, 117, 1, 19],\n",
              " [101, 102, 9, 1, 103, 104, 105, 106, 1, 107, 108],\n",
              " [1, 154, 155, 48, 153, 19, 15, 5],\n",
              " [1, 161, 2, 1, 162, 163, 19, 1, 164, 165, 166, 1, 167],\n",
              " [1, 97, 117, 1, 2, 1, 28, 3, 4, 2],\n",
              " [1, 168, 169, 170, 1, 171, 2, 172, 173],\n",
              " [1, 13, 4, 2, 1, 3, 1, 127, 128]]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, embed_size))\n",
        "for word, i in word2index.items():\n",
        "    embedding_vector = word_vectors[i]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, weights=[embedding_matrix], input_length=padded_sequences.shape[1], trainable=False),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Modeli eğitme\n",
        "model.fit(padded_sequences, np.array(labels), epochs=5, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKrCp4W1foVQ",
        "outputId": "f14f026a-8940-41e5-e0ae-57ea237a93fb"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6979 - accuracy: 0.6000\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6907 - accuracy: 0.6500\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6836 - accuracy: 0.7000\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6763 - accuracy: 0.8500\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6685 - accuracy: 0.8500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792065dc69b0>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Örnek yeni metinler\n",
        "new_texts = [\n",
        "    \"This movie was fantastic!\",\n",
        "    \"I didn't like the plot of this book.\",\n",
        "    \"The restaurant served delicious food.\",\n",
        "    \"The performance of the team was disappointing.\",\n",
        "    \"The weather ruined our picnic plans.\",\n",
        "    \"She always brightens up our day with her smile.\",\n",
        "    \"The new software update has some great features.\",\n",
        "    \"I'm really excited about my upcoming trip.\",\n",
        "    \"The traffic was unbearable on my way home.\",\n",
        "    \"The customer service experience was excellent.\"\n",
        "]\n",
        "\n",
        "# Yeni metinleri tokenlara dönüştürme\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "\n",
        "# Metinleri aynı uzunlukta doldurma (paddingleme)\n",
        "padded_new_sequences = tf.keras.preprocessing.sequence.pad_sequences(new_sequences, maxlen=15)\n",
        "\n",
        "# Model ile tahmin yapma\n",
        "predictions = model.predict(padded_new_sequences)\n",
        "\n",
        "# Tahmin sonuçlarını yazdırma\n",
        "for i, text in enumerate(new_texts):\n",
        "    sentiment = \"Positive\" if predictions[i] > 0.5 else \"Negative\"\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment} (Probability: {predictions[i][0]:.4f})\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrY5rsE8fqFN",
        "outputId": "29b0b8b4-ab85-40b5-f5ff-36ef45e84b0b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 891ms/step\n",
            "Text: This movie was fantastic!\n",
            "Predicted Sentiment: Positive (Probability: 0.5021)\n",
            "\n",
            "Text: I didn't like the plot of this book.\n",
            "Predicted Sentiment: Positive (Probability: 0.5042)\n",
            "\n",
            "Text: The restaurant served delicious food.\n",
            "Predicted Sentiment: Positive (Probability: 0.5003)\n",
            "\n",
            "Text: The performance of the team was disappointing.\n",
            "Predicted Sentiment: Negative (Probability: 0.4905)\n",
            "\n",
            "Text: The weather ruined our picnic plans.\n",
            "Predicted Sentiment: Positive (Probability: 0.5003)\n",
            "\n",
            "Text: She always brightens up our day with her smile.\n",
            "Predicted Sentiment: Negative (Probability: 0.4992)\n",
            "\n",
            "Text: The new software update has some great features.\n",
            "Predicted Sentiment: Positive (Probability: 0.5003)\n",
            "\n",
            "Text: I'm really excited about my upcoming trip.\n",
            "Predicted Sentiment: Positive (Probability: 0.5045)\n",
            "\n",
            "Text: The traffic was unbearable on my way home.\n",
            "Predicted Sentiment: Positive (Probability: 0.5004)\n",
            "\n",
            "Text: The customer service experience was excellent.\n",
            "Predicted Sentiment: Negative (Probability: 0.4959)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouRheiuQf94-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}